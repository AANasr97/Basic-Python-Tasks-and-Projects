{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1\n",
    "---------\n",
    "Scrap the USD To EGP Exchange rate from this website\n",
    "https://www.exchangerates.org.uk/Dollars-to-Egyptian-Pounds-currency-conversion-page.html\n",
    "and then use it to make a software that takes amount of USD Dollars from the user and calculate how much will it cost in EGP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-13 11:55:59 USD to EGP exchange rate: 30.9175\n",
      "100.00 USD is equivalent to 3091.75 EGP\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.exchangerates.org.uk/Dollars-to-Egyptian-Pounds-currency-conversion-page.html'\n",
    "src = requests.get(url)\n",
    "soup = BeautifulSoup(src.content, 'lxml')\n",
    "\n",
    "# find the exchange rate\n",
    "exchange_rate = soup.find('span', {'id' : 'shd2b;'}).text\n",
    "\n",
    "# convert exchange rate to a float\n",
    "exchange_rate = float(exchange_rate)\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S\"), 'USD to EGP exchange rate:', exchange_rate)\n",
    "\n",
    "# get the amount of USD dollars from the user\n",
    "usd_dollars = float(input('Enter the amount of USD dollars: '))\n",
    "\n",
    "# calculate the amount in EGP\n",
    "egp = usd_dollars * exchange_rate\n",
    "\n",
    "# print the result\n",
    "print(f'{usd_dollars:.2f} USD is equivalent to {egp:.2f} EGP')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2\n",
    "--------- \n",
    "Scraping from forecast weather\n",
    "Imagine you work for forecast weather Now imagine your boss wants you to find the temperture for each day in celsius degree. How could you do this with Beautiful Soup?\n",
    "use this web site to help you https://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Mar 11:43 PM PST     12°C\n"
     ]
    }
   ],
   "source": [
    "url = 'https://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168'\n",
    "src = requests.get(url)\n",
    "soup = BeautifulSoup(src.content, 'lxml')\n",
    "c = soup.find('p', {'class':'myforecast-current-sm'}).text\n",
    "time = soup.find_all('td')[-1].text.strip()\n",
    "\n",
    "print(time, '   ',c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3\n",
    "---------\n",
    "Scrap all books (name, price, rate) for each category and put them into a CSV & Excel file\n",
    "https://books.toscrape.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 <class 'int'>\n",
      "Done Scraping 50 Pages\n"
     ]
    }
   ],
   "source": [
    "result = requests.get('https://books.toscrape.com/index.html')\n",
    "src = result.content\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "total_pages = int(soup.find('li', {'class':'current'}).text.split('of')[1].strip())\n",
    "print(total_pages, type(total_pages))\n",
    "\n",
    "page_num = 1\n",
    "\n",
    "title = []\n",
    "price = []\n",
    "star = []\n",
    "link = []\n",
    "\n",
    "while True:\n",
    "    result = requests.get(f'https://books.toscrape.com/catalogue/page-{page_num}.html')\n",
    "    src = result.content\n",
    "    soup = BeautifulSoup(src, \"lxml\")\n",
    "\n",
    "    titles = soup.find_all('h3')\n",
    "    prices = soup.find_all('p', {'class':'price_color'})\n",
    "    stars = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    links = soup.find_all('h3')\n",
    "\n",
    "    for i in range(len(titles)):\n",
    "        title.append(titles[i].find('a').get('title'))\n",
    "        price.append(prices[i].text)\n",
    "        star.append(stars[i].select_one(\"p.star-rating\").get(\"class\")[1])\n",
    "        link.append('https://books.toscrape.com/catalogue/' + links[i].find('a').get('href'))\n",
    "        \n",
    "    \n",
    "    page_num += 1\n",
    "    \n",
    "    \n",
    "    if (page_num > total_pages):\n",
    "        print(f'Done Scraping {page_num-1} Pages')\n",
    "        break\n",
    "\n",
    "with open('1stbooks.csv', mode='a+', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Title', 'Price', 'Star Rating', 'Link'])\n",
    "        for i in range(len(title)):\n",
    "            writer.writerow([title[i], price[i], star[i], link[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Star Rating</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>Three</td>\n",
       "      <td>https://books.toscrape.com/catalogue/a-light-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>One</td>\n",
       "      <td>https://books.toscrape.com/catalogue/tipping-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>One</td>\n",
       "      <td>https://books.toscrape.com/catalogue/soumissio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>Four</td>\n",
       "      <td>https://books.toscrape.com/catalogue/sharp-obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>Five</td>\n",
       "      <td>https://books.toscrape.com/catalogue/sapiens-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Alice in Wonderland (Alice's Adventures in Won...</td>\n",
       "      <td>£55.53</td>\n",
       "      <td>One</td>\n",
       "      <td>https://books.toscrape.com/catalogue/alice-in-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Ajin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)</td>\n",
       "      <td>£57.06</td>\n",
       "      <td>Four</td>\n",
       "      <td>https://books.toscrape.com/catalogue/ajin-demi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>A Spy's Devotion (The Regency Spies of London #1)</td>\n",
       "      <td>£16.97</td>\n",
       "      <td>Five</td>\n",
       "      <td>https://books.toscrape.com/catalogue/a-spys-de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1st to Die (Women's Murder Club #1)</td>\n",
       "      <td>£53.98</td>\n",
       "      <td>One</td>\n",
       "      <td>https://books.toscrape.com/catalogue/1st-to-di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1,000 Places to See Before You Die</td>\n",
       "      <td>£26.08</td>\n",
       "      <td>Five</td>\n",
       "      <td>https://books.toscrape.com/catalogue/1000-plac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title   Price Star Rating  \\\n",
       "0                                 A Light in the Attic  £51.77       Three   \n",
       "1                                   Tipping the Velvet  £53.74         One   \n",
       "2                                           Soumission  £50.10         One   \n",
       "3                                        Sharp Objects  £47.82        Four   \n",
       "4                Sapiens: A Brief History of Humankind  £54.23        Five   \n",
       "..                                                 ...     ...         ...   \n",
       "995  Alice in Wonderland (Alice's Adventures in Won...  £55.53         One   \n",
       "996   Ajin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)  £57.06        Four   \n",
       "997  A Spy's Devotion (The Regency Spies of London #1)  £16.97        Five   \n",
       "998                1st to Die (Women's Murder Club #1)  £53.98         One   \n",
       "999                 1,000 Places to See Before You Die  £26.08        Five   \n",
       "\n",
       "                                                  Link  \n",
       "0    https://books.toscrape.com/catalogue/a-light-i...  \n",
       "1    https://books.toscrape.com/catalogue/tipping-t...  \n",
       "2    https://books.toscrape.com/catalogue/soumissio...  \n",
       "3    https://books.toscrape.com/catalogue/sharp-obj...  \n",
       "4    https://books.toscrape.com/catalogue/sapiens-a...  \n",
       "..                                                 ...  \n",
       "995  https://books.toscrape.com/catalogue/alice-in-...  \n",
       "996  https://books.toscrape.com/catalogue/ajin-demi...  \n",
       "997  https://books.toscrape.com/catalogue/a-spys-de...  \n",
       "998  https://books.toscrape.com/catalogue/1st-to-di...  \n",
       "999  https://books.toscrape.com/catalogue/1000-plac...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('1stbooks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DONT RUN THIS CODE (((IT TAKES 9 MINS))) BUT ITS WORKING :D\n",
    "\n",
    "category = []\n",
    "\n",
    "for l in link:\n",
    "    result = requests.get(l)\n",
    "    src = result.content\n",
    "    soup = BeautifulSoup(src, \"lxml\")\n",
    "    \n",
    "    category.append(soup.find('ul').text.split('\\n')[8])\n",
    "    \n",
    "    \n",
    "### DONT RUN THIS CODE (((IT TAKES 9 MINS))) BUT ITS WORKING :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('badbooks.csv', mode='a+', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Title', 'Price', 'Star Rating', 'Category', 'Link'])\n",
    "        for i in range(len(title)):\n",
    "            writer.writerow([title[i], price[i], star[i], category[i], link[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Star Rating</th>\n",
       "      <th>Category</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>Three</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>https://books.toscrape.com/catalogue/a-light-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>One</td>\n",
       "      <td>Historical Fiction</td>\n",
       "      <td>https://books.toscrape.com/catalogue/tipping-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>One</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>https://books.toscrape.com/catalogue/soumissio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>Four</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>https://books.toscrape.com/catalogue/sharp-obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>Five</td>\n",
       "      <td>History</td>\n",
       "      <td>https://books.toscrape.com/catalogue/sapiens-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Alice in Wonderland (Alice's Adventures in Won...</td>\n",
       "      <td>£55.53</td>\n",
       "      <td>One</td>\n",
       "      <td>Classics</td>\n",
       "      <td>https://books.toscrape.com/catalogue/alice-in-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Ajin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)</td>\n",
       "      <td>£57.06</td>\n",
       "      <td>Four</td>\n",
       "      <td>Sequential Art</td>\n",
       "      <td>https://books.toscrape.com/catalogue/ajin-demi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>A Spy's Devotion (The Regency Spies of London #1)</td>\n",
       "      <td>£16.97</td>\n",
       "      <td>Five</td>\n",
       "      <td>Historical Fiction</td>\n",
       "      <td>https://books.toscrape.com/catalogue/a-spys-de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1st to Die (Women's Murder Club #1)</td>\n",
       "      <td>£53.98</td>\n",
       "      <td>One</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>https://books.toscrape.com/catalogue/1st-to-di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1,000 Places to See Before You Die</td>\n",
       "      <td>£26.08</td>\n",
       "      <td>Five</td>\n",
       "      <td>Travel</td>\n",
       "      <td>https://books.toscrape.com/catalogue/1000-plac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title   Price Star Rating  \\\n",
       "0                                 A Light in the Attic  £51.77       Three   \n",
       "1                                   Tipping the Velvet  £53.74         One   \n",
       "2                                           Soumission  £50.10         One   \n",
       "3                                        Sharp Objects  £47.82        Four   \n",
       "4                Sapiens: A Brief History of Humankind  £54.23        Five   \n",
       "..                                                 ...     ...         ...   \n",
       "995  Alice in Wonderland (Alice's Adventures in Won...  £55.53         One   \n",
       "996   Ajin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)  £57.06        Four   \n",
       "997  A Spy's Devotion (The Regency Spies of London #1)  £16.97        Five   \n",
       "998                1st to Die (Women's Murder Club #1)  £53.98         One   \n",
       "999                 1,000 Places to See Before You Die  £26.08        Five   \n",
       "\n",
       "               Category                                               Link  \n",
       "0                Poetry  https://books.toscrape.com/catalogue/a-light-i...  \n",
       "1    Historical Fiction  https://books.toscrape.com/catalogue/tipping-t...  \n",
       "2               Fiction  https://books.toscrape.com/catalogue/soumissio...  \n",
       "3               Mystery  https://books.toscrape.com/catalogue/sharp-obj...  \n",
       "4               History  https://books.toscrape.com/catalogue/sapiens-a...  \n",
       "..                  ...                                                ...  \n",
       "995            Classics  https://books.toscrape.com/catalogue/alice-in-...  \n",
       "996      Sequential Art  https://books.toscrape.com/catalogue/ajin-demi...  \n",
       "997  Historical Fiction  https://books.toscrape.com/catalogue/a-spys-de...  \n",
       "998             Mystery  https://books.toscrape.com/catalogue/1st-to-di...  \n",
       "999              Travel  https://books.toscrape.com/catalogue/1000-plac...  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('badbooks.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3\n",
    "---------\n",
    "Scrap all books (name, price, rate) for each category and put them into a CSV & Excel file\n",
    "https://books.toscrape.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website to be scraped\n",
    "url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Make a GET request to fetch the raw HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "# Get all the book categories from the sidebar\n",
    "categories = soup.find('div', class_='side_categories').find_all('a')\n",
    "category_urls = [url + category['href'] for category in categories[1:]]\n",
    "# Adjust links to solve pagination\n",
    "page_num = 1\n",
    "# category_urls = [link[:-10] + f'page-{page_num}' + link[-5:] for link in category_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_links = []\n",
    "for link in category_urls:\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        soup.find('li', {'class':'current'}).text.split('of')[1].strip()\n",
    "        category_links.append(link[:-10] + f'page-{page_num}' + link[-5:])\n",
    "    except:\n",
    "        category_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel has 1 Pages  Mystery has 2 Pages  Historical Fiction has 2 Pages  Sequential Art has 4 Pages  Classics has 1 Pages  Philosophy has 1 Pages  Romance has 2 Pages  Womens Fiction has 1 Pages  Fiction has 4 Pages  Childrens has 2 Pages  Religion has 1 Pages  Nonfiction has 6 Pages  Music has 1 Pages  Default has 8 Pages  Science Fiction has 1 Pages  Sports and Games has 1 Pages  Add a comment has 4 Pages  Fantasy has 3 Pages  New Adult has 1 Pages  Young Adult has 3 Pages  Science has 1 Pages  Poetry has 1 Pages  Paranormal has 1 Pages  Art has 1 Pages  Psychology has 1 Pages  Autobiography has 1 Pages  Parenting has 1 Pages  Adult Fiction has 1 Pages  Humor has 1 Pages  Horror has 1 Pages  History has 1 Pages  Food and Drink has 2 Pages  Christian Fiction has 1 Pages  Business has 1 Pages  Biography has 1 Pages  Thriller has 1 Pages  Contemporary has 1 Pages  Spirituality has 1 Pages  Academic has 1 Pages  Self Help has 1 Pages  Historical has 1 Pages  Christian has 1 Pages  Suspense has 1 Pages  Short Stories has 1 Pages  Novels has 1 Pages  Health has 1 Pages  Politics has 1 Pages  Cultural has 1 Pages  Erotica has 1 Pages  Crime has 1 Pages  "
     ]
    }
   ],
   "source": [
    "# Loop through each category and scrape the book details\n",
    "for category_url in category_links:\n",
    "    response = requests.get(category_url)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    \n",
    "    # Get the category name\n",
    "    category = soup.find('h1').text.strip()\n",
    "    \n",
    "    try:\n",
    "        total_pages = int(soup.find('li', {'class':'current'}).text.split('of')[1].strip())\n",
    "        print(f'{category} has {total_pages} Pages', end='  ')\n",
    "    except:\n",
    "        total_pages = 1\n",
    "        print(f'{category} has {total_pages} Pages', end='  ')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the re.findall(r'\\d+', string) part.\n",
    "This uses the re.findall() function from the re module in Python to find all non-overlapping matches of one or more digits (\\d+) in the given string string. \n",
    "The result is a list of all the digit strings that were found in the original string, in the order that they appear.\n",
    "\n",
    "For example, in the given string \"Travel has 1 Pages Mystery has 2 Pages\", re.findall(r'\\d+', string) would return the list ['1', '2']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# The given string that contains numbers to be summed\n",
    "string = 'Travel has 1 Pages  Mystery has 2 Pages  Historical Fiction has 2 Pages  Sequential Art has 4 Pages  Classics has 1 Pages  Philosophy has 1 Pages  Romance has 2 Pages  Womens Fiction has 1 Pages  Fiction has 4 Pages  Childrens has 2 Pages  Religion has 1 Pages  Nonfiction has 6 Pages  Music has 1 Pages  Default has 8 Pages  Science Fiction has 1 Pages  Sports and Games has 1 Pages  Add a comment has 4 Pages  Fantasy has 3 Pages  New Adult has 1 Pages  Young Adult has 3 Pages  Science has 1 Pages  Poetry has 1 Pages  Paranormal has 1 Pages  Art has 1 Pages  Psychology has 1 Pages  Autobiography has 1 Pages  Parenting has 1 Pages  Adult Fiction has 1 Pages  Humor has 1 Pages  Horror has 1 Pages  History has 1 Pages  Food and Drink has 2 Pages  Christian Fiction has 1 Pages  Business has 1 Pages  Biography has 1 Pages  Thriller has 1 Pages  Contemporary has 1 Pages  Spirituality has 1 Pages  Academic has 1 Pages  Self Help has 1 Pages  Historical has 1 Pages  Christian has 1 Pages  Suspense has 1 Pages  Short Stories has 1 Pages  Novels has 1 Pages  Health has 1 Pages  Politics has 1 Pages  Cultural has 1 Pages  Erotica has 1 Pages  Crime has 1 Pages  '\n",
    "\n",
    "# Use regular expressions to find all non-overlapping matches of one or more digits in the string\n",
    "numbers = re.findall(r'\\d+', string)\n",
    "\n",
    "# Convert the list of digit strings to a list of integers and add them up\n",
    "sum_numbers = sum(map(int, numbers))\n",
    "\n",
    "# Print the total sum of all the numbers in the string\n",
    "print(sum_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Scraping Travel\n",
      "Done Scraping Mystery From 2 Pages\n",
      "Done Scraping Historical Fiction From 2 Pages\n",
      "Done Scraping Sequential Art From 4 Pages\n",
      "Done Scraping Classics\n",
      "Done Scraping Philosophy\n",
      "Done Scraping Romance From 2 Pages\n",
      "Done Scraping Womens Fiction\n",
      "Done Scraping Fiction From 4 Pages\n",
      "Done Scraping Childrens From 2 Pages\n",
      "Done Scraping Religion\n",
      "Done Scraping Nonfiction From 6 Pages\n",
      "Done Scraping Music\n",
      "Done Scraping Default From 8 Pages\n",
      "Done Scraping Science Fiction\n",
      "Done Scraping Sports and Games\n",
      "Done Scraping Add a comment From 4 Pages\n",
      "Done Scraping Fantasy From 3 Pages\n",
      "Done Scraping New Adult\n",
      "Done Scraping Young Adult From 3 Pages\n",
      "Done Scraping Science\n",
      "Done Scraping Poetry\n",
      "Done Scraping Paranormal\n",
      "Done Scraping Art\n",
      "Done Scraping Psychology\n",
      "Done Scraping Autobiography\n",
      "Done Scraping Parenting\n",
      "Done Scraping Adult Fiction\n",
      "Done Scraping Humor\n",
      "Done Scraping Horror\n",
      "Done Scraping History\n",
      "Done Scraping Food and Drink From 2 Pages\n",
      "Done Scraping Christian Fiction\n",
      "Done Scraping Business\n",
      "Done Scraping Biography\n",
      "Done Scraping Thriller\n",
      "Done Scraping Contemporary\n",
      "Done Scraping Spirituality\n",
      "Done Scraping Academic\n",
      "Done Scraping Self Help\n",
      "Done Scraping Historical\n",
      "Done Scraping Christian\n",
      "Done Scraping Suspense\n",
      "Done Scraping Short Stories\n",
      "Done Scraping Novels\n",
      "Done Scraping Health\n",
      "Done Scraping Politics\n",
      "Done Scraping Cultural\n",
      "Done Scraping Erotica\n",
      "Done Scraping Crime\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the scraped data and pagination start\n",
    "title = []\n",
    "price = []\n",
    "star = []\n",
    "category = []\n",
    "link = []\n",
    "\n",
    "# Loop through each category and scrape the book details\n",
    "for category_url in category_links:\n",
    "    if 'page' in category_url:\n",
    "        page_num = 1\n",
    "        while True:\n",
    "            category_url = category_url[:-6] + f'{page_num}' + category_url[-5:]\n",
    "            result = requests.get(category_url)\n",
    "            src = result.content\n",
    "            soup = BeautifulSoup(src, \"lxml\")\n",
    "            total_pages = int(soup.find('li', {'class':'current'}).text.split('of')[1].strip())\n",
    "            \n",
    "            categorys = soup.find('h1').text.strip()\n",
    "            titles = soup.find_all('h3')\n",
    "            prices = soup.find_all('p', {'class':'price_color'})\n",
    "            stars = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "            links = soup.find_all('h3')\n",
    "\n",
    "            for i in range(len(titles)):\n",
    "                title.append(titles[i].find('a').get('title'))\n",
    "                price.append(prices[i].text)\n",
    "                star.append(stars[i].select_one(\"p.star-rating\").get(\"class\")[1])\n",
    "                category.append(categorys)\n",
    "                link.append('https://books.toscrape.com/catalogue/' + links[i].find('a').get('href'))\n",
    "            \n",
    "            page_num += 1\n",
    "            \n",
    "            \n",
    "            if (page_num > total_pages):\n",
    "                print(f'Done Scraping {categorys} From {page_num-1} Pages')\n",
    "                break\n",
    "    else:\n",
    "        result = requests.get(category_url)\n",
    "        src = result.content\n",
    "        soup = BeautifulSoup(src, \"lxml\")\n",
    "        categorys = soup.find('h1').text.strip()\n",
    "        titles = soup.find_all('h3')\n",
    "        prices = soup.find_all('p', {'class':'price_color'})\n",
    "        stars = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "        links = soup.find_all('h3')\n",
    "\n",
    "        for i in range(len(titles)):\n",
    "            title.append(titles[i].find('a').get('title'))\n",
    "            price.append(prices[i].text)\n",
    "            star.append(stars[i].select_one(\"p.star-rating\").get(\"class\")[1])\n",
    "            category.append(categorys)\n",
    "            link.append('https://books.toscrape.com/catalogue/' + links[i].find('a').get('href'))\n",
    "        print(f'Done Scraping {categorys}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2ndbooks.csv', mode='a+', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Name', 'Price', 'Rating', 'Category', 'Link'])\n",
    "        for i in range(len(title)):\n",
    "            writer.writerow([title[i], price[i], star[i], category[i], link[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Category</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>Two</td>\n",
       "      <td>Travel</td>\n",
       "      <td>https://books.toscrape.com/catalogue/../../../...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Moon over Noah’s Ark: An Odyssey to Mount...</td>\n",
       "      <td>£49.43</td>\n",
       "      <td>Four</td>\n",
       "      <td>Travel</td>\n",
       "      <td>https://books.toscrape.com/catalogue/../../../...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>See America: A Celebration of Our National Par...</td>\n",
       "      <td>£48.87</td>\n",
       "      <td>Three</td>\n",
       "      <td>Travel</td>\n",
       "      <td>https://books.toscrape.com/catalogue/../../../...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vagabonding: An Uncommon Guide to the Art of L...</td>\n",
       "      <td>£36.94</td>\n",
       "      <td>Two</td>\n",
       "      <td>Travel</td>\n",
       "      <td>https://books.toscrape.com/catalogue/../../../...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Under the Tuscan Sun</td>\n",
       "      <td>£37.33</td>\n",
       "      <td>Three</td>\n",
       "      <td>Travel</td>\n",
       "      <td>https://books.toscrape.com/catalogue/../../../...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Why the Right Went Wrong: Conservatism--From G...</td>\n",
       "      <td>£52.65</td>\n",
       "      <td>Four</td>\n",
       "      <td>Politics</td>\n",
       "      <td>https://books.toscrape.com/catalogue/../../../...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Equal Is Unfair: America's Misguided Fight Aga...</td>\n",
       "      <td>£56.86</td>\n",
       "      <td>One</td>\n",
       "      <td>Politics</td>\n",
       "      <td>https://books.toscrape.com/catalogue/../../../...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Amid the Chaos</td>\n",
       "      <td>£36.58</td>\n",
       "      <td>One</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>https://books.toscrape.com/catalogue/../../../...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Dark Notes</td>\n",
       "      <td>£19.19</td>\n",
       "      <td>Five</td>\n",
       "      <td>Erotica</td>\n",
       "      <td>https://books.toscrape.com/catalogue/../../../...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>The Long Shadow of Small Ghosts: Murder and Me...</td>\n",
       "      <td>£10.97</td>\n",
       "      <td>One</td>\n",
       "      <td>Crime</td>\n",
       "      <td>https://books.toscrape.com/catalogue/../../../...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name   Price Rating  \\\n",
       "0                              It's Only the Himalayas  £45.17    Two   \n",
       "1    Full Moon over Noah’s Ark: An Odyssey to Mount...  £49.43   Four   \n",
       "2    See America: A Celebration of Our National Par...  £48.87  Three   \n",
       "3    Vagabonding: An Uncommon Guide to the Art of L...  £36.94    Two   \n",
       "4                                 Under the Tuscan Sun  £37.33  Three   \n",
       "..                                                 ...     ...    ...   \n",
       "995  Why the Right Went Wrong: Conservatism--From G...  £52.65   Four   \n",
       "996  Equal Is Unfair: America's Misguided Fight Aga...  £56.86    One   \n",
       "997                                     Amid the Chaos  £36.58    One   \n",
       "998                                         Dark Notes  £19.19   Five   \n",
       "999  The Long Shadow of Small Ghosts: Murder and Me...  £10.97    One   \n",
       "\n",
       "     Category                                               Link  \n",
       "0      Travel  https://books.toscrape.com/catalogue/../../../...  \n",
       "1      Travel  https://books.toscrape.com/catalogue/../../../...  \n",
       "2      Travel  https://books.toscrape.com/catalogue/../../../...  \n",
       "3      Travel  https://books.toscrape.com/catalogue/../../../...  \n",
       "4      Travel  https://books.toscrape.com/catalogue/../../../...  \n",
       "..        ...                                                ...  \n",
       "995  Politics  https://books.toscrape.com/catalogue/../../../...  \n",
       "996  Politics  https://books.toscrape.com/catalogue/../../../...  \n",
       "997  Cultural  https://books.toscrape.com/catalogue/../../../...  \n",
       "998   Erotica  https://books.toscrape.com/catalogue/../../../...  \n",
       "999     Crime  https://books.toscrape.com/catalogue/../../../...  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('2ndbooks.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4 (Bouns)\n",
    "---------\n",
    "### Hockey Teams Scraping\n",
    "\n",
    "https://www.scrapethissite.com/pages/forms/?page_num=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped from all 6 pages and saved to data.csv.\n"
     ]
    }
   ],
   "source": [
    "url_template = 'https://www.scrapethissite.com/pages/forms/?page_num={}&per_page=100'\n",
    "\n",
    "# Loop through all 6 pages and scrape the data\n",
    "headers = []\n",
    "rows = []\n",
    "for page_num in range(1, 7):\n",
    "    url = url_template.format(page_num)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    if not headers:\n",
    "        for th in table.find_all('th'):\n",
    "            headers.append(th.text.strip())\n",
    "    for tr in table.find_all('tr')[1:]:\n",
    "        row = []\n",
    "        for td in tr.find_all('td'):\n",
    "            row.append(td.text.strip())\n",
    "        rows.append(row)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "with open('Hockey_Teams.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(headers)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print('Data has been scraped from all 6 pages and saved to data.csv.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Losses</th>\n",
       "      <th>OT Losses</th>\n",
       "      <th>Win %</th>\n",
       "      <th>Goals For (GF)</th>\n",
       "      <th>Goals Against (GA)</th>\n",
       "      <th>+ / -</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>1990</td>\n",
       "      <td>44</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.550</td>\n",
       "      <td>299</td>\n",
       "      <td>264</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buffalo Sabres</td>\n",
       "      <td>1990</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.388</td>\n",
       "      <td>292</td>\n",
       "      <td>278</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calgary Flames</td>\n",
       "      <td>1990</td>\n",
       "      <td>46</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.575</td>\n",
       "      <td>344</td>\n",
       "      <td>263</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicago Blackhawks</td>\n",
       "      <td>1990</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.613</td>\n",
       "      <td>284</td>\n",
       "      <td>211</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Detroit Red Wings</td>\n",
       "      <td>1990</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.425</td>\n",
       "      <td>273</td>\n",
       "      <td>298</td>\n",
       "      <td>-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Tampa Bay Lightning</td>\n",
       "      <td>2011</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.463</td>\n",
       "      <td>235</td>\n",
       "      <td>281</td>\n",
       "      <td>-46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>Toronto Maple Leafs</td>\n",
       "      <td>2011</td>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.427</td>\n",
       "      <td>231</td>\n",
       "      <td>264</td>\n",
       "      <td>-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>Vancouver Canucks</td>\n",
       "      <td>2011</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.622</td>\n",
       "      <td>249</td>\n",
       "      <td>198</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Washington Capitals</td>\n",
       "      <td>2011</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.512</td>\n",
       "      <td>222</td>\n",
       "      <td>230</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>Winnipeg Jets</td>\n",
       "      <td>2011</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.451</td>\n",
       "      <td>225</td>\n",
       "      <td>246</td>\n",
       "      <td>-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Team Name  Year  Wins  Losses  OT Losses  Win %  \\\n",
       "0          Boston Bruins  1990    44      24        NaN  0.550   \n",
       "1         Buffalo Sabres  1990    31      30        NaN  0.388   \n",
       "2         Calgary Flames  1990    46      26        NaN  0.575   \n",
       "3     Chicago Blackhawks  1990    49      23        NaN  0.613   \n",
       "4      Detroit Red Wings  1990    34      38        NaN  0.425   \n",
       "..                   ...   ...   ...     ...        ...    ...   \n",
       "577  Tampa Bay Lightning  2011    38      36        8.0  0.463   \n",
       "578  Toronto Maple Leafs  2011    35      37       10.0  0.427   \n",
       "579    Vancouver Canucks  2011    51      22        9.0  0.622   \n",
       "580  Washington Capitals  2011    42      32        8.0  0.512   \n",
       "581        Winnipeg Jets  2011    37      35       10.0  0.451   \n",
       "\n",
       "     Goals For (GF)  Goals Against (GA)  + / -  \n",
       "0               299                 264     35  \n",
       "1               292                 278     14  \n",
       "2               344                 263     81  \n",
       "3               284                 211     73  \n",
       "4               273                 298    -25  \n",
       "..              ...                 ...    ...  \n",
       "577             235                 281    -46  \n",
       "578             231                 264    -33  \n",
       "579             249                 198     51  \n",
       "580             222                 230     -8  \n",
       "581             225                 246    -21  \n",
       "\n",
       "[582 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('Hockey_Teams.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Scraping\n",
    "* Countries of the World Scraping\n",
    "https://www.scrapethissite.com/pages/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the website URL\n",
    "url = 'https://www.scrapethissite.com/pages/simple/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all div elements with class 'col-md-4 country'\n",
    "countries_div = soup.find_all('div', {'class': 'col-md-4 country'})\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('countries.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Country', 'Capital', 'Population', 'Area'])\n",
    "    \n",
    "    # Extract the data for each country and write to CSV file\n",
    "    for country_div in countries_div:\n",
    "        # Extract the country name\n",
    "        country_name = country_div.find('h3').text.strip()\n",
    "        \n",
    "        # Extract the country capital\n",
    "        country_capital = country_div.find('span', {'class': 'country-capital'}).text.strip()\n",
    "        \n",
    "        # Extract the country population\n",
    "        country_population = country_div.find('span', {'class': 'country-population'}).text.strip()\n",
    "        \n",
    "        # Extract the country area\n",
    "        country_area = country_div.find('span', {'class': 'country-area'}).text.strip()\n",
    "        \n",
    "        # Write the country data to CSV file\n",
    "        writer.writerow([country_name, country_capital, country_population, country_area])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Capital</th>\n",
       "      <th>Population</th>\n",
       "      <th>Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>84000</td>\n",
       "      <td>468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>4975593</td>\n",
       "      <td>82880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Kabul</td>\n",
       "      <td>29121286</td>\n",
       "      <td>647500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>St. John's</td>\n",
       "      <td>86754</td>\n",
       "      <td>443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>The Valley</td>\n",
       "      <td>13254</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>Sanaa</td>\n",
       "      <td>23495361</td>\n",
       "      <td>527970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Mayotte</td>\n",
       "      <td>Mamoudzou</td>\n",
       "      <td>159042</td>\n",
       "      <td>374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>Pretoria</td>\n",
       "      <td>49000000</td>\n",
       "      <td>1219912.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>Lusaka</td>\n",
       "      <td>13460305</td>\n",
       "      <td>752614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Harare</td>\n",
       "      <td>11651858</td>\n",
       "      <td>390580.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Country           Capital  Population       Area\n",
       "0                 Andorra  Andorra la Vella       84000      468.0\n",
       "1    United Arab Emirates         Abu Dhabi     4975593    82880.0\n",
       "2             Afghanistan             Kabul    29121286   647500.0\n",
       "3     Antigua and Barbuda        St. John's       86754      443.0\n",
       "4                Anguilla        The Valley       13254      102.0\n",
       "..                    ...               ...         ...        ...\n",
       "245                 Yemen             Sanaa    23495361   527970.0\n",
       "246               Mayotte         Mamoudzou      159042      374.0\n",
       "247          South Africa          Pretoria    49000000  1219912.0\n",
       "248                Zambia            Lusaka    13460305   752614.0\n",
       "249              Zimbabwe            Harare    11651858   390580.0\n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('countries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Done By</summary>\n",
    "<center> Ahmed NasrElDin </center> \n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f35561962b3536e798d9d7f8aed8a6570157df2537699a33c475af9309799d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
